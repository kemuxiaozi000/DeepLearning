{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- 全连接层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1- 张量方式实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "x = tf.random.normal([2,784])\n",
    "w1 = tf.Variable(tf.random.truncated_normal([784, 256], stddev=0.1))\n",
    "b1 = tf.Variable(tf.zeros([256]))\n",
    "o1 = tf.matmul(x,w1) + b1 # 线性变换\n",
    "o1 = tf.nn.relu(o1) # 激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2- 层方式实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.random.normal([4,28*28])\n",
    "from tensorflow.keras import layers # 导入层模块\n",
    "# 创建全连接层，指定输出节点数和激活函数\n",
    "fc = layers.Dense(512, activation=tf.nn.relu) \n",
    "h1 = fc(x) # 通过 fc 类完成一次全连接层的计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'dense_5/kernel:0' shape=(784, 512) dtype=float32, numpy=\n",
       "array([[ 0.05714419, -0.02811417,  0.01908521, ...,  0.00929815,\n",
       "         0.03124575, -0.06222722],\n",
       "       [-0.03664676, -0.0166553 , -0.02162648, ..., -0.00096591,\n",
       "        -0.01241062, -0.00783542],\n",
       "       [ 0.01572664, -0.06064038,  0.01768085, ...,  0.04989342,\n",
       "         0.03935904,  0.04896101],\n",
       "       ...,\n",
       "       [-0.0269527 , -0.03488012,  0.0387957 , ...,  0.05052072,\n",
       "         0.04425357,  0.05030335],\n",
       "       [ 0.04757797,  0.01114049,  0.00188792, ..., -0.0235111 ,\n",
       "         0.04451576,  0.04620074],\n",
       "       [-0.02588462, -0.04048719,  0.00287332, ..., -0.04479849,\n",
       "         0.02461453,  0.05672096]], dtype=float32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc.kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'dense_5/bias:0' shape=(512,) dtype=float32, numpy=\n",
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- 神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1- 张量方式实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 隐藏层 1 张量\n",
    "w1 = tf.Variable(tf.random.truncated_normal([784, 256], stddev=0.1))\n",
    "b1 = tf.Variable(tf.zeros([256]))\n",
    "# 隐藏层 2 张量\n",
    "w2 = tf.Variable(tf.random.truncated_normal([256, 128], stddev=0.1))\n",
    "b2 = tf.Variable(tf.zeros([128]))\n",
    "# 隐藏层 3 张量\n",
    "w3 = tf.Variable(tf.random.truncated_normal([128, 64], stddev=0.1))\n",
    "b3 = tf.Variable(tf.zeros([64]))\n",
    "# 输出层张量\n",
    "w4 = tf.Variable(tf.random.truncated_normal([64, 10], stddev=0.1))\n",
    "b4 = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在使用 TensorFlow 自动求导功能计算梯度时，需要将前向计算过程放置在tf.GradientTape()环境中，\n",
    "# 从而利用 GradientTape 对象的 gradient()方法自动求解参数的梯度，并利用 optimizers 对象更新参数\n",
    "with tf.GradientTape() as tape: # 梯度记录器\n",
    "    # x: [b, 28*28]\n",
    "    # 隐藏层 1 前向计算，[b, 28*28] => [b, 256]\n",
    "    h1 = x@w1 + tf.broadcast_to(b1, [x.shape[0], 256])\n",
    "    h1 = tf.nn.relu(h1)\n",
    "    # 隐藏层 2 前向计算，[b, 256] => [b, 128]\n",
    "    h2 = h1@w2 + b2\n",
    "    h2 = tf.nn.relu(h2)\n",
    "    # 隐藏层 3 前向计算，[b, 128] => [b, 64] \n",
    "    h3 = h2@w3 + b3\n",
    "    h3 = tf.nn.relu(h3)\n",
    "    # 输出层前向计算，[b, 64] => [b, 10] \n",
    "    h4 = h3@w4 + b4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2- 层方式实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc1 = layers.Dense(256, activation=tf.nn.relu) # 隐藏层 1\n",
    "fc2 = layers.Dense(128, activation=tf.nn.relu) # 隐藏层 2\n",
    "fc3 = layers.Dense(64, activation=tf.nn.relu) # 隐藏层 3\n",
    "fc4 = layers.Dense(10, activation=None) # 输出层\n",
    "# 在前向计算时，依序通过各个网络层即可：\n",
    "x = tf.random.normal([4,28*28])\n",
    "h1 = fc1(x) # 通过隐藏层 1 得到输出\n",
    "h2 = fc2(h1) # 通过隐藏层 2 得到输出\n",
    "h3 = fc3(h2) # 通过隐藏层 3 得到输出\n",
    "h4 = fc4(h3) # 通过输出层得到网络输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 也可以通过 Sequential 容器封装成一个网络大类对象，调用大类的前向计算函数\n",
    "from tensorflow.keras import Sequential\n",
    "model = Sequential([\n",
    "     layers.Dense(256, activation=tf.nn.relu) , # 创建隐藏层 1\n",
    "     layers.Dense(128, activation=tf.nn.relu) , # 创建隐藏层 2\n",
    "     layers.Dense(64, activation=tf.nn.relu) , # 创建隐藏层 3\n",
    "     layers.Dense(10, activation=None) , # 创建输出层\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3- 激励函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.nn.sigmoid  \n",
    "tf.nn.relu  \n",
    "tf.nn.leaky_relu  \n",
    "tf.nn.tanh  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4- [0,1]区间，和为 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=634, shape=(3,), dtype=float32, numpy=array([0.6590012, 0.242433 , 0.0985659], dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 输出层添加 Softmax 函数实现。\n",
    "z = tf.constant([2.,1.,0.1])\n",
    "tf.nn.softmax(z) # 通过 Softmax 函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=680, shape=(), dtype=float32, numpy=2.456634>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"在 Softmax 函数的数值计算过程中，容易因输入值偏大发生数值溢出现象；在计算交\n",
    "叉熵时，也会出现数值溢出的问题。为了数值计算的稳定性，TensorFlow 中提供了一个统\n",
    "一的接口，将 Softmax 与交叉熵损失函数同时实现，同时也处理了数值不稳定的异常，一\n",
    "般推荐使用，避免单独使用 Softmax 函数与交叉熵损失函数。\"\"\"\n",
    "z = tf.random.normal([2,10]) # 构造输出层的输出\n",
    "y_onehot = tf.constant([1,3]) # 构造真实值\n",
    "y_onehot = tf.one_hot(y_onehot, depth=10) # one-hot 编码\n",
    "# 输出层未使用 Softmax 函数，故 from_logits 设置为 True\n",
    "loss = keras.losses.categorical_crossentropy(y_onehot,z,from_logits=True)\n",
    "loss = tf.reduce_mean(loss) # 计算平均交叉熵损失\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5- 误差计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 均方差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 交叉熵\n",
    "熵越大，代表不确定性越大，信息量也就越大。 tf.math.log  \n",
    "交叉熵可以很好地衡量 2 个分布之间的差别，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
