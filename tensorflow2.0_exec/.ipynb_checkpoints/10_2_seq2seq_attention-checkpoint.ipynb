{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. preprocessing data\\n2. build model\\n2.1 encoder\\n2.2 attention\\n2.3 decoder\\n3.evalution\\n3.1 given sentence, return translated result\\n3.2 visualize results(attention)\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. preprocessing data\n",
    "2. build model\n",
    "2.1 encoder\n",
    "2.2 attention\n",
    "2.3 decoder\n",
    "3.evalution\n",
    "3.1 given sentence, return translated result\n",
    "3.2 visualize results(attention)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-beta1\n",
      "sys.version_info(major=3, minor=6, micro=7, releaselevel='final', serial=0)\n",
      "matplotlib 3.1.2\n",
      "numpy 1.16.2\n",
      "pandas 0.25.3\n",
      "sklearn 0.22\n",
      "tensorflow 2.0.0-beta1\n",
      "tensorflow.python.keras.api._v2.keras 2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "print(sys.version_info)\n",
    "for module in mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Put it on\n",
      "Poneoslo\n"
     ]
    }
   ],
   "source": [
    "en_spa_file_path = \"./data/spa-eng/spa.txt\"\n",
    "\n",
    "import unicodedata\n",
    "# 转成asc是为了减小词表\n",
    "def unicode_to_ascii(s):\n",
    "    # NFD 如果有一个unicode是多个asc组成的，就把这个拆开， Mn 重音\n",
    "    return \"\".join(c for c in unicodedata.normalize(\"NFD\",s) if unicodedata.category(c) != \"Mn\")\n",
    "\n",
    "en_sentence = \"Put it on\"\n",
    "sp_sentence = \"Ponéoslo\"\n",
    "# 比如é 是一个e和一个重音符号，分开因为重音符号是Mn，所以忽略 所以é => e\n",
    "print(unicode_to_ascii(en_sentence))\n",
    "print(unicode_to_ascii(sp_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> put it on <end>\n",
      "<start> poneoslo <end>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def preprocess_sentence(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    # []任意一个， （）替换操作, \\1 本身\n",
    "    # 标点符号前后加空格\n",
    "    s = re.sub(r\"([?.!,¿])\", r\" \\1 \", s)\n",
    "    # 多余空格变成一个空格\n",
    "    s = re.sub(r'[\" \"]+', \" \", s)\n",
    "    # 除了标点符号和字母以外都是空格\n",
    "    s = re.sub(r'[^a-zA-Z?.!,¿]+', \" \", s)\n",
    "    # 去掉前后空格\n",
    "    s = s.rstrip().strip()\n",
    "    s = '<start> ' + s + ' <end>'\n",
    "    return s\n",
    "\n",
    "def make_list(x,y):\n",
    "    return [x,y]\n",
    "\n",
    "print(preprocess_sentence(en_sentence))\n",
    "print(preprocess_sentence(sp_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> it may be impossible to get a completely error free corpus due to the nature of this kind of collaborative effort . however , if we encourage members to contribute sentences in their own languages rather than experiment in languages they are learning , we might be able to minimize errors . <end>\n",
      "<start> puede que sea imposible obtener un corpus completamente libre de errores debido a la naturaleza de este tipo de esfuerzo de colaboracion . sin embargo , si animamos a los miembros a contribuir frases en sus propios idiomas en lugar de experimentar con los idiomas que estan aprendiendo , podriamos ser capaces de minimizar los errores . <end>\n"
     ]
    }
   ],
   "source": [
    "def parse_data(filename):\n",
    "    lines = open(filename, encoding=\"UTF-8\").read().strip().split(\"\\n\")\n",
    "    sentence_pairs = [line.split(\"\\t\")[0:-1] for line in lines]\n",
    "    preprocessed_sentence_pairs = [ (preprocess_sentence(en),preprocess_sentence(sp)) for en, sp  in sentence_pairs]\n",
    "    return zip(*preprocessed_sentence_pairs)\n",
    "en_dataset, sp_dataset = parse_data(en_spa_file_path)\n",
    "print(en_dataset[-1])\n",
    "print(sp_dataset[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2) (3, 4) (5, 6)\n",
      "(1, 3, 5) (2, 4, 6)\n"
     ]
    }
   ],
   "source": [
    "# 补充\n",
    "a = [(1,2), (3,4),(5,6)]\n",
    "# 单星号能够将这个变量拆分成单个元素\n",
    "print(*a)\n",
    "# zip可以转置\n",
    "c,d = zip(*a)\n",
    "print(c,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 11\n",
      "[16, 16, 16, 16, 16, 16, 16, 16, 16, 16] [11, 11, 11, 11, 11, 11, 11, 11, 11, 11]\n"
     ]
    }
   ],
   "source": [
    "# 文本式数据要被model读取要变成id式\n",
    "def tokenizer(lang):\n",
    "    lang_tokenizer = keras.preprocessing.text.Tokenizer(\n",
    "        num_words = None, filters=\"\", split=\" \"\n",
    "    )\n",
    "    \n",
    "    lang_tokenizer.fit_on_texts(lang) \n",
    "    #序列的列表，列表中每个序列对应于一段输入文本 \n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang) #得到词索引[[1, 2, 3, 4], [1, 2, 3, 5]]\n",
    "    tensor = keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                   padding = \"post\")\n",
    "    return tensor, lang_tokenizer\n",
    "input_tensor, input_tokenizer = tokenizer(sp_dataset[0:30000])\n",
    "output_tensor, output_tokenizer = tokenizer(en_dataset[0:30000])\n",
    "\n",
    "\n",
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)\n",
    "\n",
    "def len_test(tensor):\n",
    "    return [len(t) for t in tensor[0:10]]\n",
    "\n",
    "max_len_input = max_length(input_tensor)\n",
    "max_len_output = max_length(output_tensor)\n",
    "\n",
    "print(max_len_input, max_len_output)\n",
    "print(len_test(input_tensor), len_test(output_tensor))\n",
    "# dir(input_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24000, 6000, 24000, 6000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 切分训练集和验证集\n",
    "from sklearn.model_selection import train_test_split\n",
    "input_train, input_eval, output_train, output_eval = train_test_split(input_tensor, \n",
    "                                                                      output_tensor, test_size=0.2)\n",
    "\n",
    "len(input_train), len(input_eval), len(output_train), len(output_eval) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 --> <start>\n",
      "53 --> eres\n",
      "39 --> muy\n",
      "706 --> valiente\n",
      "3 --> .\n",
      "2 --> <end>\n",
      "\n",
      "1 --> <start>\n",
      "5 --> you\n",
      "25 --> are\n",
      "49 --> very\n",
      "550 --> brave\n",
      "3 --> .\n",
      "2 --> <end>\n"
     ]
    }
   ],
   "source": [
    "# 验证tokenizer是否起作用\n",
    "def convert(example, tokenizer):\n",
    "    for t in example:\n",
    "        if t != 0:\n",
    "            print(\"%d --> %s\" %(t, tokenizer.index_word[t]))\n",
    "\n",
    "convert(input_train[1], input_tokenizer)\n",
    "print()\n",
    "convert(output_train[1], output_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(input_tensor, output_tensor, \n",
    "                 batch_size, epochs, shuffle):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((input_tensor, \n",
    "                                                 output_tensor))\n",
    "    if shuffle:\n",
    "        dataset.shuffle(30000)\n",
    "    dataset = dataset.repeat(epochs).batch(batch_size, \n",
    "                                           drop_remainder=True)\n",
    "    return dataset\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 20\n",
    "    \n",
    "train_dataset = make_dataset(input_train, output_train,\n",
    "                             batch_size, epochs, True)\n",
    "eval_dataset = make_dataset(input_eval, output_eval,\n",
    "                             batch_size, 1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 16)\n",
      "(64, 11)\n",
      "tf.Tensor(\n",
      "[[  1   5  22 ...   0   0   0]\n",
      " [  1  53  39 ...   0   0   0]\n",
      " [  1   6 613 ...   0   0   0]\n",
      " ...\n",
      " [  1   8 604 ...   0   0   0]\n",
      " [  1  28 648 ...   0   0   0]\n",
      " [  1   5  16 ...   0   0   0]], shape=(64, 16), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[   1   79   22    5   22    9    6    2    0    0    0]\n",
      " [   1    5   25   49  550    3    2    0    0    0    0]\n",
      " [   1    7   99   10   67  261    3    2    0    0    0]\n",
      " [   1  189   25   34 1142    3    2    0    0    0    0]\n",
      " [   1  124   55   13  761    3    2    0    0    0    0]\n",
      " [   1    7   23   12   74    5    3    2    0    0    0]\n",
      " [   1   31   76  187    3    2    0    0    0    0    0]\n",
      " [   1   40    5   43   47    6    2    0    0    0    0]\n",
      " [   1   79   40    7   22    9    6    2    0    0    0]\n",
      " [   1   79   22    5   95   39    6    2    0    0    0]\n",
      " [   1    7 1098   46    3    2    0    0    0    0    0]\n",
      " [   1   30   98    8   48    3    2    0    0    0    0]\n",
      " [   1    4   35 1975    3    2    0    0    0    0    0]\n",
      " [   1    4   23   12   95   80    7    3    2    0    0]\n",
      " [   1  171   71   71  225    3    2    0    0    0    0]\n",
      " [   1   32   10 2579   37    2    0    0    0    0    0]\n",
      " [   1    4   73   19  797    3    2    0    0    0    0]\n",
      " [   1    4   23   12  271    9    3    2    0    0    0]\n",
      " [   1    7    8  160   39    3    2    0    0    0    0]\n",
      " [   1  290   37    2    0    0    0    0    0    0    0]\n",
      " [   1    7  598 4580    3    2    0    0    0    0    0]\n",
      " [   1   22    5   29    9    6    2    0    0    0    0]\n",
      " [   1    4   57  263    3    2    0    0    0    0    0]\n",
      " [   1    4   23   12  909   71    3    2    0    0    0]\n",
      " [   1    4   16   72  479 1835    3    2    0    0    0]\n",
      " [   1   19    8   65   83    3    2    0    0    0    0]\n",
      " [   1   14 2213   10   67  313    3    2    0    0    0]\n",
      " [   1   19   11  708    3    2    0    0    0    0    0]\n",
      " [   1    4   16  102  458    3    2    0    0    0    0]\n",
      " [   1   25    5  635    6    2    0    0    0    0    0]\n",
      " [   1    4  109 4736   21 4737    3    2    0    0    0]\n",
      " [   1   87 2455    3    2    0    0    0    0    0    0]\n",
      " [   1    4  114    9   11  277    3    2    0    0    0]\n",
      " [   1    7  209  232    3    2    0    0    0    0    0]\n",
      " [   1   44   11  147    3    2    0    0    0    0    0]\n",
      " [   1    4   27   12   35  211  278    3    2    0    0]\n",
      " [   1   29  140  375    3    2    0    0    0    0    0]\n",
      " [   1   32   40    7  125   18    6    2    0    0    0]\n",
      " [   1   28   24  231    3    2    0    0    0    0    0]\n",
      " [   1    4   69  627   54    3    2    0    0    0    0]\n",
      " [   1   68   69    4    6    2    0    0    0    0    0]\n",
      " [   1    4   35 2870    3    2    0    0    0    0    0]\n",
      " [   1   68    8   13  334    6    2    0    0    0    0]\n",
      " [   1   44   18   29   10  111    3    2    0    0    0]\n",
      " [   1   14   11 2472    3    2    0    0    0    0    0]\n",
      " [   1   56   11   36   19   83    6    2    0    0    0]\n",
      " [   1    8   19  574    6    2    0    0    0    0    0]\n",
      " [   1    7  444   82   10  151    3    2    0    0    0]\n",
      " [   1  275   18   30 1911   37    2    0    0    0    0]\n",
      " [   1   32   11   30  135    6    2    0    0    0    0]\n",
      " [   1    7  709   70  258    3    2    0    0    0    0]\n",
      " [   1   88   58   20  706    3    2    0    0    0    0]\n",
      " [   1   68   11   13  369    6    2    0    0    0    0]\n",
      " [   1    4   38   41  301    3    2    0    0    0    0]\n",
      " [   1   28   91  184    3    2    0    0    0    0    0]\n",
      " [   1   31   11   21  413    3    2    0    0    0    0]\n",
      " [   1   28  475   15  147    3    2    0    0    0    0]\n",
      " [   1    9  304  367  120    3    2    0    0    0    0]\n",
      " [   1    8   20  146    6    2    0    0    0    0    0]\n",
      " [   1   52  113   36    3    2    0    0    0    0    0]\n",
      " [   1   32   11   19 2713    6    2    0    0    0    0]\n",
      " [   1   28  100   12   52   90    3    2    0    0    0]\n",
      " [   1   19  590  892    3    2    0    0    0    0    0]\n",
      " [   1    8    7  458    6    2    0    0    0    0    0]], shape=(64, 11), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_dataset.take(1):\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    print(x)\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9403\n",
      "4834\n"
     ]
    }
   ],
   "source": [
    "# 定义超参数\n",
    "embedding_units = 256\n",
    "units = 1024\n",
    "input_vocab_size = len(input_tokenizer.word_index)+1\n",
    "output_vocab_size = len(output_tokenizer.word_index)+1\n",
    "\n",
    "print(input_vocab_size)\n",
    "print(output_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_output.shape: (64, 16, 1024)\n",
      "sample_hidden.shape: (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "class Encoder(keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_units, encoding_units, batch_size):\n",
    "    # 使用super().__init__()手动执行父类的构造方法, \n",
    "    # 不然会由于子类重写父类的__init__的方法导致父类在构造方法中定义的默认属性无法继承（不能使用）\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.encoding_units = encoding_units\n",
    "        # embedding将大型稀疏向量转换为保留语义关系的低维空间\n",
    "        self.embedding = keras.layers.Embedding(vocab_size,\n",
    "                                                embedding_units)\n",
    "        self.gru = keras.layers.GRU(self.encoding_units, \n",
    "                                    return_sequences= True,\n",
    "                                   return_state = True,\n",
    "                                   recurrent_initializer=\"glorot_uniform\")\n",
    "    def call(self, x, hidden):\n",
    "        # 函数式调用\n",
    "        # a Layer instance is callable on a tensor , and returns a tensor \n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x,initial_state = hidden)\n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size, self.encoding_units))\n",
    "encoder = Encoder(input_vocab_size, embedding_units,\n",
    "                  units, batch_size )\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(x, sample_hidden)\n",
    "\n",
    "print(\"sample_output.shape:\", sample_output.shape)\n",
    "print(\"sample_hidden.shape:\", sample_hidden.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-15-83c2746ad144>, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-15-83c2746ad144>\"\u001b[1;36m, line \u001b[1;32m8\u001b[0m\n\u001b[1;33m    def call(self.decoder_hidden, encoder_outputs):\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class BahdanauAttention(keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = keras.layers.Dense(units)\n",
    "        self.W2 = keras.layers.Dense(units)\n",
    "        self.V = keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self.decoder_hidden, encoder_outputs):\n",
    "        # dencoder_hidden.shape: (batch_size, units)\n",
    "        # encoder_outputs.shape: (batch_size, length, units)\n",
    "        \n",
    "        # before V: (batch_size, length, units)\n",
    "        # after V: (batch_size, length, 1)\n",
    "        # tf.expand_dims:在指定索引出增加一维度，值为1，从索引0开始\n",
    "        # axis: 取值范围是[-阶数，阶数]，二维的时候0指的是列，1指的是行，\n",
    "        decoder_hidden_with_time_axis = tf.expend_dims(decoder_hidden, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(encoder_outputs) + self.W2(decoder_hidden_with_time_axis)))\n",
    "        \n",
    "        # shape: (batch_size, length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis = 1)\n",
    "        \n",
    "        # 加权\n",
    "        # context_vector.shape: (batch_size, length, units)\n",
    "        context_vector = attention_weights * encoder_outputs\n",
    "        \n",
    "        # 平均  在length 维度上求和\n",
    "        #  context_vector.shape: (batch_size, units)\n",
    "        context_vector = tf.reduce_sum(context_vector, axis = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
