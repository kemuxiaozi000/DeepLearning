{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.3.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import  warnings \n",
    "warnings . filterwarnings ( 'ignore' )\n",
    "\n",
    "import  keras \n",
    "keras . __version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  keras.layers  import  Embedding\n",
    "\n",
    "#嵌入层( Embedding layer)的构建至少需要两个参数：\n",
    "#可能的符标(token)的数量，这里是1000(1 + maximum word index), \n",
    "#和嵌入(embedding)的维度，这里是64。\n",
    "\n",
    "embedding_layer  =  Embedding ( 1000 ,  64 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 4s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from  keras.datasets  import  imdb \n",
    "from  keras  import  preprocessing\n",
    "\n",
    "#要考虑作为特征的单词数\n",
    "max_features  =  10000\n",
    "\n",
    "#在此单词数量之后剪切文本\n",
    "maxlen  =  20\n",
    "\n",
    "#将数据加载为整数列表\n",
    "( x_train ,  y_train ),  ( x_test ,  y_test )  =  imdb . load_data ( num_words = max_features )\n",
    "\n",
    "#这将我们的整数列表变成一个2D整个张量(samples, maxlen) \n",
    "x_train  =  preprocessing . sequence . pad_sequences ( x_train ,  maxlen = maxlen )  #不够长的补'0',太长的裁剪掉\n",
    "x_test  =  preprocessing . sequence . pad_sequences ( x_test ,  maxlen = maxlen )  #不够长的补'0',太长的裁剪掉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 20)\n",
      "(25000, 20)\n"
     ]
    }
   ],
   "source": [
    "print ( x_train . shape ) \n",
    "print ( x_test . shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 嵌入层(embedding layer)\n",
    "最好的理解是: 将整数索引（代表特定单词）映射到密集向量的字典物件中。输入整数，它查找一个内部字典，并返回相关的向量。\n",
    "\n",
    "嵌入层以(samples, sequence_length)的整数2D张量作为输入，其中每笔资料是一个整数序列（sequence of integers)。它也可以嵌入不同长度的序列，例如，我们可以将这些序列输入到可能具有形状（32,10）（一批32个长度10）或（64,15）的批次之上的嵌入层中（一批64个长度15）。虽然批处理中的所有序列必须具有相同的长度（因为我们需要将它们打包为单个张量），所以比其他序列短的序列应该用零填充，并且应该截断较长的序列。\n",
    "\n",
    "这个嵌入层会返回一个形状为（samples，sequence_length，embedding_dimensionality）的3D浮点张量。然后这样的3D张量由RNN层或1D卷积层处理。\n",
    "\n",
    "当我们实例化一个Embedding层的时候，它的权重（它的内部符标向量字典）最初是随机的，就像任何其他神经层。在训练过程中，这些单词向量将通过反向传播逐渐调整，将嵌入空间构建成一些下游模式可以利用的东西。一旦经过充分训练，我们的嵌入空间将显示出许多结构(一种专门针对我们正在训练模型的特定问题结构)。\n",
    "\n",
    "让我们将这个想法应用于IMDB电影评论情绪预测任务。让我们快速准备数据。我们会将电影评论限制在最常见的10,000个单词中，并且仅在20个单词后删减评论。我们的网络将简单地学习10,000个单词中的每一个的8维嵌入，将输入整数序列（2D整数张量）转化为嵌入序列（3D浮点张量），将张量平坦化为2D，并训练一个单独的Dense层来进行分类。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 20, 8)             80000     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 161       \n",
      "=================================================================\n",
      "Total params: 80,161\n",
      "Trainable params: 80,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 3s 133us/step - loss: 0.6798 - acc: 0.5954 - val_loss: 0.6431 - val_acc: 0.6806\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 2s 99us/step - loss: 0.5622 - acc: 0.7438 - val_loss: 0.5348 - val_acc: 0.7308\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 2s 101us/step - loss: 0.4667 - acc: 0.7868 - val_loss: 0.5012 - val_acc: 0.7444\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 2s 97us/step - loss: 0.4222 - acc: 0.8094 - val_loss: 0.4930 - val_acc: 0.7522\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 2s 105us/step - loss: 0.3930 - acc: 0.8242 - val_loss: 0.4933 - val_acc: 0.7544\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 2s 98us/step - loss: 0.3695 - acc: 0.8376 - val_loss: 0.4963 - val_acc: 0.7544\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 2s 99us/step - loss: 0.3486 - acc: 0.8504 - val_loss: 0.5003 - val_acc: 0.7522\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 2s 98us/step - loss: 0.3284 - acc: 0.8608 - val_loss: 0.5077 - val_acc: 0.7504\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 2s 98us/step - loss: 0.3093 - acc: 0.8718 - val_loss: 0.5130 - val_acc: 0.7520\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 2s 100us/step - loss: 0.2909 - acc: 0.8809 - val_loss: 0.5219 - val_acc: 0.7502\n"
     ]
    }
   ],
   "source": [
    "from  keras.models  import  Sequential \n",
    "from  keras.layers  import  Flatten ,  Dense\n",
    "\n",
    "model  =  Sequential ()\n",
    "\n",
    "#我们为嵌入层指定最大输入长度，以便稍后将嵌入式输入平坦化\n",
    "#参数：\n",
    "#符标(token)的数量，这里是1000 \n",
    "#嵌入(embedding)的维度，这里是8 \n",
    "model . add ( Embedding ( 10000 ,  8 ,  input_length = maxlen ))\n",
    "\n",
    "# 在嵌入层之后，我们的张量形状转换成为`(samples, maxlen, 8)`.\n",
    "\n",
    "#我们将3D嵌入张量变成2D张量形状`(samples, maxlen * 8)` \n",
    "model . add ( Flatten ())\n",
    "\n",
    "#我们添加一个二元分类层\n",
    "model . add ( Dense ( 1 ,  activation = 'sigmoid' ))\n",
    "\n",
    "model . compile ( optimizer = 'rmsprop' ,  loss = 'binary_crossentropy' ,  metrics = [ 'acc' ]) \n",
    "model . summary ()\n",
    "\n",
    "history  =  model . fit ( x_train ,  y_train , \n",
    "                    epochs = 10 , \n",
    "                    batch_size = 32 , \n",
    "                    validation_split = 0.2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
