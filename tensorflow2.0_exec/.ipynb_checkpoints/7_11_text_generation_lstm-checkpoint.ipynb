{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-beta1\n",
      "sys.version_info(major=3, minor=6, micro=7, releaselevel='final', serial=0)\n",
      "matplotlib 3.1.2\n",
      "numpy 1.16.2\n",
      "pandas 0.25.3\n",
      "sklearn 0.22\n",
      "tensorflow 2.0.0-beta1\n",
      "tensorflow.python.keras.api._v2.keras 2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "print(sys.version_info)\n",
    "for module in mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A\\.keras\\datasets\\shakespeare.txt\n",
      "1115394\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "path_to_file = tf.keras.utils.get_file(\n",
    "    'shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "print(path_to_file)\n",
    "text = open(path_to_file, \"r\").read()\n",
    "print(len(text))\n",
    "print(text[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. generate vocab\n",
    "# 2. build mapping  char -> id\n",
    "# 3. data -> id data\n",
    "# 4. 预测下一个字符的模型  abcd - > bcd<eos>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. generate vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. build mapping  char <=> id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n"
     ]
    }
   ],
   "source": [
    "# enumerate() 会对每一个元素生成一个index,\n",
    "char2idx = {char :idx for idx,char in enumerate(vocab) }\n",
    "print(char2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n' ' ' '!' '$' '&' \"'\" ',' '-' '.' '3' ':' ';' '?' 'A' 'B' 'C' 'D' 'E'\n",
      " 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U' 'V' 'W'\n",
      " 'X' 'Y' 'Z' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o'\n",
      " 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z']\n"
     ]
    }
   ],
   "source": [
    "idx2char = np.array(vocab)\n",
    "print(idx2char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. data => id data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43\n",
      "  1 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43\n",
      " 39 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49\n",
      "  6  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10\n",
      "  0 37 53 59]\n"
     ]
    }
   ],
   "source": [
    "text_as_int = np.array([char2idx[c] for c in text])\n",
    "print(text_as_int[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. make dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(18, shape=(), dtype=int32) F\n",
      "tf.Tensor(47, shape=(), dtype=int32) i\n",
      "tf.Tensor(\n",
      "[18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43\n",
      "  1 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43\n",
      " 39 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49\n",
      "  6  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10\n",
      "  0 37 53 59  1], shape=(101,), dtype=int32)\n",
      "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "tf.Tensor(\n",
      "[39 56 43  1 39 50 50  1 56 43 57 53 50 60 43 42  1 56 39 58 46 43 56  1\n",
      " 58 53  1 42 47 43  1 58 46 39 52  1 58 53  1 44 39 51 47 57 46 12  0  0\n",
      " 13 50 50 10  0 30 43 57 53 50 60 43 42  8  1 56 43 57 53 50 60 43 42  8\n",
      "  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 18 47 56 57 58  6  1\n",
      " 63 53 59  1 49], shape=(101,), dtype=int32)\n",
      "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n"
     ]
    }
   ],
   "source": [
    "# 样本抽取,用dataset的方式\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "seq_length = 100\n",
    "seq_dataset = char_dataset.batch(seq_length + 1, \n",
    "                                 drop_remainder = True)\n",
    "\n",
    "for ch_id in char_dataset.take(2):\n",
    "    print(ch_id, idx2char[ch_id.numpy()])\n",
    "\n",
    "for seq_id in seq_dataset.take(2):\n",
    "    print(seq_id)\n",
    "#     print(\"\".join(idx2char[seq_id.numpy()]))\n",
    "    print(repr(\"\".join(idx2char[seq_id.numpy()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. dataset中的每个batch(0:100) 拆分成（0：99）和（1：100）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把字符集分配成输入和输出，作为训练样本\n",
    "def split_input_target(id_text):\n",
    "    \"\"\"\n",
    "    abcde -> abcd,bcde\n",
    "    \"\"\"\n",
    "    return id_text[0:-1], id_text[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43\n",
      "  1 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43\n",
      " 39 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49\n",
      "  6  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10\n",
      "  0 37 53 59]\n",
      "[47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43  1\n",
      " 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43 39\n",
      " 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49  6\n",
      "  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10  0\n",
      " 37 53 59  1]\n",
      "[39 56 43  1 39 50 50  1 56 43 57 53 50 60 43 42  1 56 39 58 46 43 56  1\n",
      " 58 53  1 42 47 43  1 58 46 39 52  1 58 53  1 44 39 51 47 57 46 12  0  0\n",
      " 13 50 50 10  0 30 43 57 53 50 60 43 42  8  1 56 43 57 53 50 60 43 42  8\n",
      "  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 18 47 56 57 58  6  1\n",
      " 63 53 59  1]\n",
      "[56 43  1 39 50 50  1 56 43 57 53 50 60 43 42  1 56 39 58 46 43 56  1 58\n",
      " 53  1 42 47 43  1 58 46 39 52  1 58 53  1 44 39 51 47 57 46 12  0  0 13\n",
      " 50 50 10  0 30 43 57 53 50 60 43 42  8  1 56 43 57 53 50 60 43 42  8  0\n",
      "  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 18 47 56 57 58  6  1 63\n",
      " 53 59  1 49]\n"
     ]
    }
   ],
   "source": [
    "seq_dataset = seq_dataset.map(split_input_target)\n",
    "\n",
    "for item_input, item_output in seq_dataset.take(2):\n",
    "    print(item_input.numpy())\n",
    "    print(item_output.numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. 模型建立"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "buffer_size = 10000\n",
    "\n",
    "seq_dataset = seq_dataset.shuffle(buffer_size).batch(\n",
    "    batch_size, drop_remainder = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n",
      "WARNING:tensorflow:Entity <function standard_lstm at 0x000002248EC509D8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_lstm at 0x000002248EC509D8>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function standard_lstm at 0x000002248EC509D8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_lstm at 0x000002248EC509D8>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <function cudnn_lstm at 0x000002248EC50A60> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_lstm at 0x000002248EC50A60>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function cudnn_lstm at 0x000002248EC50A60> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_lstm at 0x000002248EC50A60>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           16640     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (64, None, 1024)          5246976   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 65)            66625     \n",
      "=================================================================\n",
      "Total params: 5,330,241\n",
      "Trainable params: 5,330,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(text[0:100])\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024\n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Embedding(vocab_size, embedding_dim, \n",
    "                               batch_input_shape=[batch_size, None]),\n",
    "        keras.layers.LSTM(units = rnn_units,\n",
    "                          recurrent_initializer='glorot_uniform',\n",
    "                          stateful=True,\n",
    "                          return_sequences = True),\n",
    "        keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model\n",
    "model = build_model( vocab_size = vocab_size,\n",
    "                     embedding_dim = embedding_dim,\n",
    "                     rnn_units = rnn_units,\n",
    "                     batch_size = batch_size)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return keras.losses.sparse_categorical_crossentropy(labels, \n",
    "                                                      logits, \n",
    "                                                      from_logits = True)\n",
    "\n",
    "model.compile(optimizer = \"adam\", loss = loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "172/172 [==============================] - 13s 75ms/step - loss: 2.6412\n",
      "Epoch 2/100\n",
      "172/172 [==============================] - 11s 62ms/step - loss: 1.9301\n",
      "Epoch 3/100\n",
      "172/172 [==============================] - 10s 60ms/step - loss: 1.6743\n",
      "Epoch 4/100\n",
      "172/172 [==============================] - 10s 61ms/step - loss: 1.5355\n",
      "Epoch 5/100\n",
      "172/172 [==============================] - 11s 63ms/step - loss: 1.4489\n",
      "Epoch 6/100\n",
      "172/172 [==============================] - 11s 63ms/step - loss: 1.3880\n",
      "Epoch 7/100\n",
      "172/172 [==============================] - 11s 63ms/step - loss: 1.3402\n",
      "Epoch 8/100\n",
      "172/172 [==============================] - 11s 63ms/step - loss: 1.2980\n",
      "Epoch 9/100\n",
      "172/172 [==============================] - 11s 63ms/step - loss: 1.2589\n",
      "Epoch 10/100\n",
      "172/172 [==============================] - 11s 65ms/step - loss: 1.2220\n",
      "Epoch 11/100\n",
      "172/172 [==============================] - 11s 65ms/step - loss: 1.1872\n",
      "Epoch 12/100\n",
      "172/172 [==============================] - 11s 64ms/step - loss: 1.1527\n",
      "Epoch 13/100\n",
      "172/172 [==============================] - 11s 65ms/step - loss: 1.1208\n",
      "Epoch 14/100\n",
      "172/172 [==============================] - 11s 63ms/step - loss: 1.0912\n",
      "Epoch 15/100\n",
      "172/172 [==============================] - 11s 63ms/step - loss: 1.0596\n",
      "Epoch 16/100\n",
      "172/172 [==============================] - 11s 63ms/step - loss: 1.0272\n",
      "Epoch 17/100\n",
      "172/172 [==============================] - 11s 64ms/step - loss: 0.9999\n",
      "Epoch 18/100\n",
      "172/172 [==============================] - 11s 65ms/step - loss: 0.9705\n",
      "Epoch 19/100\n",
      "172/172 [==============================] - 11s 63ms/step - loss: 0.9367\n",
      "Epoch 20/100\n",
      "172/172 [==============================] - 11s 63ms/step - loss: 0.9038\n",
      "Epoch 21/100\n",
      "172/172 [==============================] - 11s 64ms/step - loss: 0.8778\n",
      "Epoch 22/100\n",
      "172/172 [==============================] - 11s 62ms/step - loss: 0.8525\n",
      "Epoch 23/100\n",
      "172/172 [==============================] - 11s 66ms/step - loss: 0.8273\n",
      "Epoch 24/100\n",
      "172/172 [==============================] - 11s 66ms/step - loss: 0.8038\n",
      "Epoch 25/100\n",
      "172/172 [==============================] - 11s 66ms/step - loss: 0.7801\n",
      "Epoch 26/100\n",
      "172/172 [==============================] - 11s 65ms/step - loss: 0.7553\n",
      "Epoch 27/100\n",
      "172/172 [==============================] - 11s 64ms/step - loss: 0.7332\n",
      "Epoch 28/100\n",
      "172/172 [==============================] - 11s 66ms/step - loss: 0.7129\n",
      "Epoch 29/100\n",
      "172/172 [==============================] - 11s 66ms/step - loss: 0.6942\n",
      "Epoch 30/100\n",
      "172/172 [==============================] - 11s 65ms/step - loss: 0.6737\n",
      "Epoch 31/100\n",
      "172/172 [==============================] - 11s 63ms/step - loss: 0.6563\n",
      "Epoch 32/100\n",
      "172/172 [==============================] - 11s 64ms/step - loss: 0.6408\n",
      "Epoch 33/100\n",
      "172/172 [==============================] - 11s 65ms/step - loss: 0.6276\n",
      "Epoch 34/100\n",
      "172/172 [==============================] - 11s 65ms/step - loss: 0.6134\n",
      "Epoch 35/100\n",
      "172/172 [==============================] - 11s 66ms/step - loss: 0.6017\n",
      "Epoch 36/100\n",
      " 18/172 [==>...........................] - ETA: 18s - loss: 0.6102"
     ]
    }
   ],
   "source": [
    "output_dir = \".text_generation_lstm_checkpoints\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "checkpoint_prefix = os.path.join(output_dir, \"ckpt_{epoch}\")\n",
    "checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath = checkpoint_prefix,\n",
    "#     monitor='val_loss',\n",
    "#     verbose=0,\n",
    "#     save_best_only=False,\n",
    "    save_weights_only=True,\n",
    "#     mode='auto',\n",
    "#     save_freq='epoch',\n",
    "#     load_weights_on_restart=False\n",
    ")\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "history = model.fit(seq_dataset, epochs = epochs,\n",
    "                   callbacks= [checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. 重载模型（利用checkpoint）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./text_generation_checkpoints\"\n",
    "tf.train.latest_checkpoint(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model2 = build_model(vocab_size = vocab_size,\n",
    "                     embedding_dim = embedding_dim,\n",
    "                     rnn_units = rnn_units,\n",
    "                     batch_size = 1)\n",
    "model2.load_weights(tf.train.latest_checkpoint(output_dir))\n",
    "model2.build(tf.TensorShape([1,None]))\n",
    "\n",
    "# start ch sequence A,\n",
    "# A -> model -> b\n",
    "# A.append(b) -> B\n",
    "# B -> model-> c\n",
    "# B.append(c) -> C\n",
    "model2.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model,start_string, num_generate = 1000):\n",
    "    input_eval = [char2idx[ch] for ch in start_string]\n",
    "    input_eval= tf.expand_dims(input_eval, 0)\n",
    "    \n",
    "    text_generated = []\n",
    "    model.reset_states()\n",
    "    \n",
    "    temperature = 0.5\n",
    "    \n",
    "    for _ in range(num_generate):\n",
    "        # 1.model inference ->predictions\n",
    "        # 2.sample\n",
    "        # 3.update input_eval\n",
    "        \n",
    "        # predictions : [batch_size, input_eval_len, vocab_size]\n",
    "        predictions = model(input_eval)\n",
    "        # temperature 作用\n",
    "        # predictions : logits -> softmax -> prob\n",
    "        # softmax: e^xi\n",
    "        # eg: 4,2   e^4/(e^4 + e^2) = 0.88   e^2/(e^4 + e^2) = 0.12\n",
    "        #     2,1   e^2/(e^1 + e^2) = 0.73   e^1/(e^1 + e^2) = 0.27\n",
    "        predictions = predictions / temperature\n",
    "        \n",
    "        # predictions : [input_eval_len, vocab_size]   消掉batch_size的维度\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        # predicetd_ids : [input_eval_len, 1]\n",
    "        # a b c -> b c d ,只用最后一个 predicetd_ids 就是d\n",
    "        predicted_id = tf.random.categorical(\n",
    "            predictions, num_samples = 1\n",
    "        )[-1,0].numpy()\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "        # 为什么不在input_eval后面添加predicted_id，而是直接替换  => 为了防止低效\n",
    "        # s, x -> rnn -> s', y  \n",
    "        input_eval = tf.expand_dims([predicted_id],0)\n",
    "    return start_string + \"\".join(text_generated)\n",
    "new_text = generate_text(model2, \"All: \")\n",
    "print(new_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text_1 = generate_text(model2, \"First Citizen\")\n",
    "print(new_text_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "All: good my lord, and so much leising kind Fruntage;\n",
    "When Tyrreat exile her maid at Montague.\n",
    "A coward, loving correction. When thou mayst nothing else\n",
    "By patience and I know by men:\n",
    "Who increase thou of my mateous daughter,\n",
    "Yor nothing chide as double at your own.\n",
    "\n",
    "KING RICHARD III:\n",
    "Here come the Lady Bohemiage, but were still\n",
    "door eat and a homeful dinner sound imprison'd;\n",
    "But no more senators, and they sad\n",
    "some success of power to speak again concrain,\n",
    "Harl power and brothers: and they shall power\n",
    "That thou art so least and respites from gentle seat,\n",
    "Who do apparelt in the secret message\n",
    "That seem like progar of that vengeance one!\n",
    "O Friend, with mine, that torthy priest\n",
    "Meant with us sweet and procedged strikes\n",
    "Are as time to teach and services be contrary,\n",
    "And eart--\n",
    "Comm, death, for death my duty milder stones,\n",
    "If it possesses on the queen's blood of them,\n",
    "And spit at all; good sorrow he doth approve\n",
    "As sweet as shunble love to assising solier:\n",
    "When he so say, that I should keep to g\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "First Citizen:\n",
    "Before some we will. I shall be your good voices this\n",
    "verier'd here, that rights that honourably in\n",
    "Time little to a fine sun.\n",
    "\n",
    "CLARENCE:\n",
    "I mut a deal of person mouth the vantage.\n",
    "Thou liberty much lend a fork:\n",
    "Therefore impose not me: the cannot choose.\n",
    "Come Hastings, whence comes but well, sure the rest,\n",
    "Revolt our fathers are forth from Deepet:\n",
    "But let me have a copil-hamed perfect\n",
    "lay know the royalty of a good duke of God!\n",
    "\n",
    "GLOUCESTER:\n",
    "Up, do not so, but yet metware\n",
    "The Emperor of this place of intesters' sake.\n",
    "\n",
    "GREMIO:\n",
    "Warwick, think'st thou, Nattle, and thy change pronounce\n",
    "Edmard Cominius to Bolingbroke's proud joy?\n",
    "When, join'd togars drown day son?\n",
    "The tiger'st undergeant to my wife,\n",
    "I think there be right shops as bit with sword;\n",
    "Which I have given away with thee and thy arms,\n",
    "In this my leave of Bories and York and Sir Sheal'd for her lives\n",
    "Before thy service and so look'd together:\n",
    "The words of wondrous sun to Pradut Shall,\n",
    "Obstant my woman.\n",
    "\n",
    "BIONDELLO:\n",
    "I took the can wa\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
