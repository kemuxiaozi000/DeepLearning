{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-beta1\n",
      "sys.version_info(major=3, minor=6, micro=7, releaselevel='final', serial=0)\n",
      "matplotlib 3.1.2\n",
      "numpy 1.16.2\n",
      "pandas 0.25.3\n",
      "sklearn 0.22\n",
      "tensorflow 2.0.0-beta1\n",
      "tensorflow.python.keras.api._v2.keras 2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)\n",
    "print(sys.version_info)\n",
    "for module in mpl, np, pd, sklearn, tf, keras:\n",
    "    print(module.__name__, module.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\A\\.keras\\datasets\\shakespeare.txt\n",
      "1115394\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "path_to_file = tf.keras.utils.get_file(\n",
    "    'shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "print(path_to_file)\n",
    "text = open(path_to_file, \"r\").read()\n",
    "print(len(text))\n",
    "print(text[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. generate vocab\n",
    "# 2. build mapping  char -> id\n",
    "# 3. data -> id data\n",
    "# 4. 预测下一个字符的模型  abcd - > bcd<eos>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. generate vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. build mapping  char <=> id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n"
     ]
    }
   ],
   "source": [
    "# enumerate() 会对每一个元素生成一个index,\n",
    "char2idx = {char :idx for idx,char in enumerate(vocab) }\n",
    "print(char2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n' ' ' '!' '$' '&' \"'\" ',' '-' '.' '3' ':' ';' '?' 'A' 'B' 'C' 'D' 'E'\n",
      " 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U' 'V' 'W'\n",
      " 'X' 'Y' 'Z' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n' 'o'\n",
      " 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z']\n"
     ]
    }
   ],
   "source": [
    "idx2char = np.array(vocab)\n",
    "print(idx2char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. data => id data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43\n",
      "  1 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43\n",
      " 39 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49\n",
      "  6  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10\n",
      "  0 37 53 59]\n"
     ]
    }
   ],
   "source": [
    "text_as_int = np.array([char2idx[c] for c in text])\n",
    "print(text_as_int[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. make dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(18, shape=(), dtype=int32) F\n",
      "tf.Tensor(47, shape=(), dtype=int32) i\n",
      "tf.Tensor(\n",
      "[18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43\n",
      "  1 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43\n",
      " 39 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49\n",
      "  6  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10\n",
      "  0 37 53 59  1], shape=(101,), dtype=int32)\n",
      "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "tf.Tensor(\n",
      "[39 56 43  1 39 50 50  1 56 43 57 53 50 60 43 42  1 56 39 58 46 43 56  1\n",
      " 58 53  1 42 47 43  1 58 46 39 52  1 58 53  1 44 39 51 47 57 46 12  0  0\n",
      " 13 50 50 10  0 30 43 57 53 50 60 43 42  8  1 56 43 57 53 50 60 43 42  8\n",
      "  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 18 47 56 57 58  6  1\n",
      " 63 53 59  1 49], shape=(101,), dtype=int32)\n",
      "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n"
     ]
    }
   ],
   "source": [
    "# 样本抽取,用dataset的方式\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "seq_length = 100\n",
    "seq_dataset = char_dataset.batch(seq_length + 1, \n",
    "                                 drop_remainder = True)\n",
    "\n",
    "for ch_id in char_dataset.take(2):\n",
    "    print(ch_id, idx2char[ch_id.numpy()])\n",
    "\n",
    "for seq_id in seq_dataset.take(2):\n",
    "    print(seq_id)\n",
    "#     print(\"\".join(idx2char[seq_id.numpy()]))\n",
    "    print(repr(\"\".join(idx2char[seq_id.numpy()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. dataset中的每个batch(0:100) 拆分成（0：99）和（1：100）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把字符集分配成输入和输出，作为训练样本\n",
    "def split_input_target(id_text):\n",
    "    \"\"\"\n",
    "    abcde -> abcd,bcde\n",
    "    \"\"\"\n",
    "    return id_text[0:-1], id_text[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43\n",
      "  1 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43\n",
      " 39 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49\n",
      "  6  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10\n",
      "  0 37 53 59]\n",
      "[47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43  1\n",
      " 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43 39\n",
      " 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49  6\n",
      "  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10  0\n",
      " 37 53 59  1]\n",
      "[39 56 43  1 39 50 50  1 56 43 57 53 50 60 43 42  1 56 39 58 46 43 56  1\n",
      " 58 53  1 42 47 43  1 58 46 39 52  1 58 53  1 44 39 51 47 57 46 12  0  0\n",
      " 13 50 50 10  0 30 43 57 53 50 60 43 42  8  1 56 43 57 53 50 60 43 42  8\n",
      "  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 18 47 56 57 58  6  1\n",
      " 63 53 59  1]\n",
      "[56 43  1 39 50 50  1 56 43 57 53 50 60 43 42  1 56 39 58 46 43 56  1 58\n",
      " 53  1 42 47 43  1 58 46 39 52  1 58 53  1 44 39 51 47 57 46 12  0  0 13\n",
      " 50 50 10  0 30 43 57 53 50 60 43 42  8  1 56 43 57 53 50 60 43 42  8  0\n",
      "  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 18 47 56 57 58  6  1 63\n",
      " 53 59  1 49]\n"
     ]
    }
   ],
   "source": [
    "seq_dataset = seq_dataset.map(split_input_target)\n",
    "\n",
    "for item_input, item_output in seq_dataset.take(2):\n",
    "    print(item_input.numpy())\n",
    "    print(item_output.numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. 模型建立"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "buffer_size = 10000\n",
    "\n",
    "seq_dataset = seq_dataset.shuffle(buffer_size).batch(\n",
    "    batch_size, drop_remainder = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n",
      "WARNING:tensorflow:Entity <function standard_lstm at 0x00000188437F09D8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_lstm at 0x00000188437F09D8>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function standard_lstm at 0x00000188437F09D8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_lstm at 0x00000188437F09D8>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <function cudnn_lstm at 0x00000188437F0A60> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_lstm at 0x00000188437F0A60>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function cudnn_lstm at 0x00000188437F0A60> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_lstm at 0x00000188437F0A60>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           16640     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (64, None, 1024)          5246976   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 65)            66625     \n",
      "=================================================================\n",
      "Total params: 5,330,241\n",
      "Trainable params: 5,330,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(text[0:100])\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 256\n",
    "rnn_units = 1024\n",
    "\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Embedding(vocab_size, embedding_dim, \n",
    "                               batch_input_shape=[batch_size, None]),\n",
    "        keras.layers.LSTM(units = rnn_units,\n",
    "                          recurrent_initializer='glorot_uniform',\n",
    "                          stateful=True,\n",
    "                          return_sequences = True),\n",
    "        keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model\n",
    "model = build_model( vocab_size = vocab_size,\n",
    "                     embedding_dim = embedding_dim,\n",
    "                     rnn_units = rnn_units,\n",
    "                     batch_size = batch_size)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return keras.losses.sparse_categorical_crossentropy(labels, \n",
    "                                                      logits, \n",
    "                                                      from_logits = True)\n",
    "\n",
    "model.compile(optimizer = \"adam\", loss = loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "172/172 [==============================] - 13s 75ms/step - loss: 2.5805\n",
      "Epoch 2/100\n",
      "172/172 [==============================] - 11s 63ms/step - loss: 1.8759\n",
      "Epoch 3/100\n",
      "172/172 [==============================] - 11s 63ms/step - loss: 1.6287\n",
      "Epoch 4/100\n",
      "172/172 [==============================] - 11s 65ms/step - loss: 1.4950\n",
      "Epoch 5/100\n",
      "172/172 [==============================] - 11s 65ms/step - loss: 1.4122\n",
      "Epoch 6/100\n",
      "172/172 [==============================] - 11s 65ms/step - loss: 1.3511\n",
      "Epoch 7/100\n",
      "172/172 [==============================] - 11s 65ms/step - loss: 1.3005\n",
      "Epoch 8/100\n",
      "172/172 [==============================] - 11s 63ms/step - loss: 1.2533\n",
      "Epoch 9/100\n",
      "172/172 [==============================] - 11s 65ms/step - loss: 1.2075\n",
      "Epoch 10/100\n",
      "172/172 [==============================] - 11s 64ms/step - loss: 1.1631\n",
      "Epoch 11/100\n",
      "172/172 [==============================] - 11s 64ms/step - loss: 1.1226\n",
      "Epoch 12/100\n",
      "172/172 [==============================] - 11s 64ms/step - loss: 1.0867\n",
      "Epoch 13/100\n",
      "172/172 [==============================] - 11s 65ms/step - loss: 1.0516\n",
      "Epoch 14/100\n",
      "172/172 [==============================] - 11s 63ms/step - loss: 1.0160\n",
      "Epoch 15/100\n",
      "172/172 [==============================] - 11s 66ms/step - loss: 0.9789\n",
      "Epoch 16/100\n",
      "172/172 [==============================] - 11s 65ms/step - loss: 0.9438\n",
      "Epoch 17/100\n",
      "172/172 [==============================] - 11s 64ms/step - loss: 0.9105\n",
      "Epoch 18/100\n",
      "172/172 [==============================] - 11s 64ms/step - loss: 0.8815\n",
      "Epoch 19/100\n",
      "172/172 [==============================] - 11s 64ms/step - loss: 0.8520\n",
      "Epoch 20/100\n",
      "172/172 [==============================] - 11s 64ms/step - loss: 0.8210\n",
      "Epoch 21/100\n",
      "172/172 [==============================] - 11s 64ms/step - loss: 0.7934\n",
      "Epoch 22/100\n",
      "172/172 [==============================] - 11s 64ms/step - loss: 0.7700\n",
      "Epoch 23/100\n",
      "172/172 [==============================] - 11s 63ms/step - loss: 0.7473\n",
      "Epoch 24/100\n",
      "172/172 [==============================] - 11s 64ms/step - loss: 0.7244\n",
      "Epoch 25/100\n",
      "172/172 [==============================] - 11s 64ms/step - loss: 0.7049\n",
      "Epoch 26/100\n",
      "172/172 [==============================] - 11s 64ms/step - loss: 0.6910\n",
      "Epoch 27/100\n",
      "172/172 [==============================] - 11s 63ms/step - loss: 0.6761\n",
      "Epoch 28/100\n",
      "172/172 [==============================] - 11s 63ms/step - loss: 0.6578\n",
      "Epoch 29/100\n",
      "172/172 [==============================] - 11s 64ms/step - loss: 0.6377\n",
      "Epoch 30/100\n",
      "172/172 [==============================] - 11s 63ms/step - loss: 0.6158\n",
      "Epoch 31/100\n",
      "172/172 [==============================] - 11s 64ms/step - loss: 0.5971\n",
      "Epoch 32/100\n",
      "172/172 [==============================] - 11s 64ms/step - loss: 0.5773\n",
      "Epoch 33/100\n",
      "172/172 [==============================] - 11s 63ms/step - loss: 0.5605\n",
      "Epoch 34/100\n",
      "172/172 [==============================] - 11s 67ms/step - loss: 0.5458\n",
      "Epoch 35/100\n",
      "172/172 [==============================] - 12s 67ms/step - loss: 0.5328\n",
      "Epoch 36/100\n",
      "172/172 [==============================] - 12s 67ms/step - loss: 0.5189\n",
      "Epoch 37/100\n",
      "172/172 [==============================] - 12s 67ms/step - loss: 0.5084\n",
      "Epoch 38/100\n",
      "172/172 [==============================] - 12s 68ms/step - loss: 0.4986\n",
      "Epoch 39/100\n",
      "172/172 [==============================] - 12s 68ms/step - loss: 0.4885\n",
      "Epoch 40/100\n",
      "172/172 [==============================] - 12s 70ms/step - loss: 0.4784\n",
      "Epoch 41/100\n",
      "172/172 [==============================] - 12s 69ms/step - loss: 0.4687\n",
      "Epoch 42/100\n",
      "172/172 [==============================] - 12s 69ms/step - loss: 0.4578\n",
      "Epoch 43/100\n",
      "172/172 [==============================] - 12s 72ms/step - loss: 0.4505\n",
      "Epoch 44/100\n",
      "172/172 [==============================] - 12s 69ms/step - loss: 0.4471\n",
      "Epoch 45/100\n",
      "172/172 [==============================] - 12s 69ms/step - loss: 0.4423\n",
      "Epoch 46/100\n",
      "172/172 [==============================] - 12s 68ms/step - loss: 0.4325\n",
      "Epoch 47/100\n",
      "172/172 [==============================] - 11s 67ms/step - loss: 0.4286\n",
      "Epoch 48/100\n",
      "172/172 [==============================] - 11s 66ms/step - loss: 0.4214\n",
      "Epoch 49/100\n",
      "172/172 [==============================] - 11s 66ms/step - loss: 0.4142\n",
      "Epoch 50/100\n",
      "172/172 [==============================] - 11s 66ms/step - loss: 0.4108\n",
      "Epoch 51/100\n",
      "172/172 [==============================] - 11s 66ms/step - loss: 0.4053\n",
      "Epoch 52/100\n",
      "172/172 [==============================] - 12s 67ms/step - loss: 0.4008\n",
      "Epoch 53/100\n",
      "172/172 [==============================] - 12s 69ms/step - loss: 0.3926\n",
      "Epoch 54/100\n",
      "172/172 [==============================] - 11s 66ms/step - loss: 0.3876\n",
      "Epoch 55/100\n",
      "172/172 [==============================] - 11s 65ms/step - loss: 0.3821\n",
      "Epoch 56/100\n",
      "172/172 [==============================] - 11s 65ms/step - loss: 0.3780\n",
      "Epoch 57/100\n",
      "172/172 [==============================] - 11s 67ms/step - loss: 0.3707\n",
      "Epoch 58/100\n",
      "172/172 [==============================] - 11s 66ms/step - loss: 0.3681\n",
      "Epoch 59/100\n",
      "172/172 [==============================] - 12s 67ms/step - loss: 0.3637\n",
      "Epoch 60/100\n",
      "172/172 [==============================] - 12s 68ms/step - loss: 0.3609\n",
      "Epoch 61/100\n",
      "172/172 [==============================] - 12s 69ms/step - loss: 0.3603\n",
      "Epoch 62/100\n",
      "172/172 [==============================] - 12s 67ms/step - loss: 0.3586\n",
      "Epoch 63/100\n",
      "172/172 [==============================] - 11s 67ms/step - loss: 0.3554\n",
      "Epoch 64/100\n",
      "172/172 [==============================] - 12s 69ms/step - loss: 0.3536\n",
      "Epoch 65/100\n",
      "172/172 [==============================] - 12s 68ms/step - loss: 0.3486 1\n",
      "Epoch 66/100\n",
      "172/172 [==============================] - 11s 67ms/step - loss: 0.3430\n",
      "Epoch 67/100\n",
      "172/172 [==============================] - 12s 69ms/step - loss: 0.3394\n",
      "Epoch 68/100\n",
      "172/172 [==============================] - 12s 68ms/step - loss: 0.3398\n",
      "Epoch 69/100\n",
      "172/172 [==============================] - 12s 69ms/step - loss: 0.3379\n",
      "Epoch 70/100\n",
      "172/172 [==============================] - 12s 70ms/step - loss: 0.3299\n",
      "Epoch 71/100\n",
      "172/172 [==============================] - 12s 69ms/step - loss: 0.3303\n",
      "Epoch 72/100\n",
      "172/172 [==============================] - 12s 70ms/step - loss: 0.3254\n",
      "Epoch 73/100\n",
      "172/172 [==============================] - 12s 69ms/step - loss: 0.3246\n",
      "Epoch 74/100\n",
      "172/172 [==============================] - 12s 70ms/step - loss: 0.3210\n",
      "Epoch 75/100\n",
      "172/172 [==============================] - 12s 70ms/step - loss: 0.3180\n",
      "Epoch 76/100\n",
      "172/172 [==============================] - 12s 69ms/step - loss: 0.3162\n",
      "Epoch 77/100\n",
      "172/172 [==============================] - 12s 69ms/step - loss: 0.3126 0s - loss: \n",
      "Epoch 78/100\n",
      "172/172 [==============================] - 12s 68ms/step - loss: 0.3073 0s -\n",
      "Epoch 79/100\n",
      "172/172 [==============================] - 12s 70ms/step - loss: 0.3068\n",
      "Epoch 80/100\n",
      "172/172 [==============================] - 12s 69ms/step - loss: 0.3068\n",
      "Epoch 81/100\n",
      "172/172 [==============================] - 12s 69ms/step - loss: 0.3033\n",
      "Epoch 82/100\n",
      "172/172 [==============================] - 12s 69ms/step - loss: 0.3036\n",
      "Epoch 83/100\n",
      "172/172 [==============================] - 12s 70ms/step - loss: 0.3041\n",
      "Epoch 84/100\n",
      "172/172 [==============================] - 12s 69ms/step - loss: 0.3011\n",
      "Epoch 85/100\n",
      "172/172 [==============================] - 12s 70ms/step - loss: 0.2989\n",
      "Epoch 86/100\n",
      "172/172 [==============================] - 12s 71ms/step - loss: 0.3012\n",
      "Epoch 87/100\n",
      "172/172 [==============================] - 12s 70ms/step - loss: 0.2985\n",
      "Epoch 88/100\n",
      "172/172 [==============================] - 12s 69ms/step - loss: 0.2938\n",
      "Epoch 89/100\n",
      "172/172 [==============================] - 12s 70ms/step - loss: 0.2906\n",
      "Epoch 90/100\n",
      "172/172 [==============================] - 12s 68ms/step - loss: 0.2854\n",
      "Epoch 91/100\n",
      "172/172 [==============================] - 12s 68ms/step - loss: 0.2834\n",
      "Epoch 92/100\n",
      "172/172 [==============================] - 12s 70ms/step - loss: 0.2854\n",
      "Epoch 93/100\n",
      "172/172 [==============================] - 12s 70ms/step - loss: 0.2850\n",
      "Epoch 94/100\n",
      "172/172 [==============================] - 12s 69ms/step - loss: 0.2903\n",
      "Epoch 95/100\n",
      "172/172 [==============================] - 12s 70ms/step - loss: 0.2912\n",
      "Epoch 96/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172/172 [==============================] - 12s 68ms/step - loss: 0.2872\n",
      "Epoch 97/100\n",
      "172/172 [==============================] - 12s 70ms/step - loss: 0.2871\n",
      "Epoch 98/100\n",
      "172/172 [==============================] - 12s 69ms/step - loss: 0.2856\n",
      "Epoch 99/100\n",
      "172/172 [==============================] - 12s 70ms/step - loss: 0.2819\n",
      "Epoch 100/100\n",
      "172/172 [==============================] - 12s 71ms/step - loss: 0.2796\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"./text_generation_lstm_checkpoints\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "checkpoint_prefix = os.path.join(output_dir, \"ckpt_{epoch}\")\n",
    "checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath = checkpoint_prefix,\n",
    "#     monitor='val_loss',\n",
    "#     verbose=0,\n",
    "#     save_best_only=False,\n",
    "    save_weights_only=True,\n",
    "#     mode='auto',\n",
    "#     save_freq='epoch',\n",
    "#     load_weights_on_restart=False\n",
    ")\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "history = model.fit(seq_dataset, epochs = epochs,\n",
    "                   callbacks= [checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. 重载模型（利用checkpoint）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./text_generation_lstm_checkpoints\\\\ckpt_100'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <function standard_lstm at 0x00000188437F09D8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_lstm at 0x00000188437F09D8>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function standard_lstm at 0x00000188437F09D8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function standard_lstm at 0x00000188437F09D8>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <function cudnn_lstm at 0x00000188437F0A60> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_lstm at 0x00000188437F0A60>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function cudnn_lstm at 0x00000188437F0A60> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cudnn_lstm at 0x00000188437F0A60>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            16640     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (1, None, 1024)           5246976   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 65)             66625     \n",
      "=================================================================\n",
      "Total params: 5,330,241\n",
      "Trainable params: 5,330,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model2 = build_model(vocab_size = vocab_size,\n",
    "                     embedding_dim = embedding_dim,\n",
    "                     rnn_units = rnn_units,\n",
    "                     batch_size = 1)\n",
    "model2.load_weights(tf.train.latest_checkpoint(output_dir))\n",
    "model2.build(tf.TensorShape([1,None]))\n",
    "\n",
    "# start ch sequence A,\n",
    "# A -> model -> b\n",
    "# A.append(b) -> B\n",
    "# B -> model-> c\n",
    "# B.append(c) -> C\n",
    "model2.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All: and my conceit ir\n",
      "speak not to be the coll the war;\n",
      "And what that is not yet standing 'tword, as this?\n",
      "\n",
      "quite a bod of lord, within this work,\n",
      "That thou didst cry all for the present death.\n",
      "I fear, by thy spirit to him, I had rather my\n",
      "brother; the bound of Edward's cruppe or two fortune tide thy eyes\n",
      "Of noble essimation and well gractly,\n",
      "iccass I see her; for once more leisure\n",
      "In every scene so much well as she;\n",
      "why, there's a word. Come, madam wife to her\n",
      "And unrusted to our waters and so strong add\n",
      "provoke me to my stuff.\n",
      "\n",
      "Page:\n",
      "I'll see you on his knowledge.\n",
      "\n",
      "AUFIDIUS:\n",
      "My liege, I cannot do thee now, I had put us it finds,\n",
      "Even to the firm of s he did die brow\n",
      "Thy knowle guard with God,\n",
      "Doth hunt ble heir to the drunkard let him do it.\n",
      "\n",
      "LADY CAPULET:\n",
      "O heart, about me: these I would have take to put you to\n",
      "no further.\n",
      "\n",
      "VIRGILIA:\n",
      "No, good madam,--\n",
      "\n",
      "PAULINA:\n",
      "For which their head saitors, suffer it\n",
      "To see your grave to be your queen, let them have\n",
      "That vowburse in this all?\n",
      "\n",
      "Allowers \n"
     ]
    }
   ],
   "source": [
    "def generate_text(model,start_string, num_generate = 1000):\n",
    "    input_eval = [char2idx[ch] for ch in start_string]\n",
    "    input_eval= tf.expand_dims(input_eval, 0)\n",
    "    \n",
    "    text_generated = []\n",
    "    model.reset_states()\n",
    "    \n",
    "    # temperature > 1, 概率分布更平滑  随意\n",
    "    # temperature < 1, 概率分布更陡峭  greedy\n",
    "    temperature = 0.5\n",
    "    \n",
    "    for _ in range(num_generate):\n",
    "        # 1.model inference ->predictions\n",
    "        # 2.sample\n",
    "        # 3.update input_eval\n",
    "        \n",
    "        # predictions : [batch_size, input_eval_len, vocab_size]\n",
    "        predictions = model(input_eval)\n",
    "        # temperature 作用\n",
    "        # predictions : logits -> softmax -> prob\n",
    "        # softmax: e^xi\n",
    "        # eg: 4,2   e^4/(e^4 + e^2) = 0.88   e^2/(e^4 + e^2) = 0.12\n",
    "        #     2,1   e^2/(e^1 + e^2) = 0.73   e^1/(e^1 + e^2) = 0.27\n",
    "        predictions = predictions / temperature\n",
    "        \n",
    "        # predictions : [input_eval_len, vocab_size]   消掉batch_size的维度\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        # predicetd_ids : [input_eval_len, 1]\n",
    "        # a b c -> b c d ,只用最后一个 predicetd_ids 就是d\n",
    "        predicted_id = tf.random.categorical(\n",
    "            predictions, num_samples = 1\n",
    "        )[-1,0].numpy()\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "        # 为什么不在input_eval后面添加predicted_id，而是直接替换  => 为了防止低效\n",
    "        # s, x -> rnn -> s', y  \n",
    "        input_eval = tf.expand_dims([predicted_id],0)\n",
    "    return start_string + \"\".join(text_generated)\n",
    "new_text = generate_text(model2, \"All: \")\n",
    "print(new_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Nay, now I see thee shame to make you guard with heaven\n",
      "Would buy think it for them to work, he has here.\n",
      "\n",
      "GRUMIO:\n",
      "Katharina, you, lords!\n",
      "My child o' the silent hour of your couse,\n",
      "Live me thy merry melt the prond knowledge\n",
      "The ladely day, that he hath done me with her age.\n",
      "Swere arm, we will be a judge; but I can read\n",
      "waiting justice of his advirence; but it is George\n",
      "Save me to chase in te that public day!\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "By Him that raised un?\n",
      "\n",
      "SICINIUS:\n",
      "Say you so?\n",
      "\n",
      "AUFIDIUS:\n",
      "No more!\n",
      "\n",
      "COMINIUS:\n",
      "You shall not be awhieved.\n",
      "\n",
      "PAULINA:\n",
      "I care not:\n",
      "Good faith, I'll prove him.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "A hundred cast our good will but a lord!\n",
      "\n",
      "GLOUCESTER:\n",
      "Now is the wind that follow this forward!\n",
      "\n",
      "First Servant:\n",
      "My lord?\n",
      "\n",
      "LEONTES:\n",
      "The gods here did stand for consul, which nd thou\n",
      "art a\n",
      "nothing make the damal's place,\n",
      "Drawn and be patiently to greet young Paris have been a\n",
      "formen to the earth\n",
      "Or either partyrous to me. Let me take thou move of thy followers\n",
      "Are the very pity: mise upplied join\n"
     ]
    }
   ],
   "source": [
    "new_text_1 = generate_text(model2, \"First Citizen\")\n",
    "print(new_text_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n100次 没有\\nAll: good my lord, and so much leising kind Fruntage;\\nWhen Tyrreat exile her maid at Montague.\\nA coward, loving correction. When thou mayst nothing else\\nBy patience and I know by men:\\nWho increase thou of my mateous daughter,\\nYor nothing chide as double at your own.\\n\\nKING RICHARD III:\\nHere come the Lady Bohemiage, but were still\\ndoor eat and a homeful dinner sound imprison'd;\\nBut no more senators, and they sad\\nsome success of power to speak again concrain,\\nHarl power and brothers: and they shall power\\nThat thou art so least and respites from gentle seat,\\nWho do apparelt in the secret message\\nThat seem like progar of that vengeance one!\\nO Friend, with mine, that torthy priest\\nMeant with us sweet and procedged strikes\\nAre as time to teach and services be contrary,\\nAnd eart--\\nComm, death, for death my duty milder stones,\\nIf it possesses on the queen's blood of them,\\nAnd spit at all; good sorrow he doth approve\\nAs sweet as shunble love to assising solier:\\nWhen he so say, that I should keep to g\\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "100次 没有\n",
    "All: good my lord, and so much leising kind Fruntage;\n",
    "When Tyrreat exile her maid at Montague.\n",
    "A coward, loving correction. When thou mayst nothing else\n",
    "By patience and I know by men:\n",
    "Who increase thou of my mateous daughter,\n",
    "Yor nothing chide as double at your own.\n",
    "\n",
    "KING RICHARD III:\n",
    "Here come the Lady Bohemiage, but were still\n",
    "door eat and a homeful dinner sound imprison'd;\n",
    "But no more senators, and they sad\n",
    "some success of power to speak again concrain,\n",
    "Harl power and brothers: and they shall power\n",
    "That thou art so least and respites from gentle seat,\n",
    "Who do apparelt in the secret message\n",
    "That seem like progar of that vengeance one!\n",
    "O Friend, with mine, that torthy priest\n",
    "Meant with us sweet and procedged strikes\n",
    "Are as time to teach and services be contrary,\n",
    "And eart--\n",
    "Comm, death, for death my duty milder stones,\n",
    "If it possesses on the queen's blood of them,\n",
    "And spit at all; good sorrow he doth approve\n",
    "As sweet as shunble love to assising solier:\n",
    "When he so say, that I should keep to g\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nFirst Citizen:\\nBefore some we will. I shall be your good voices this\\nverier'd here, that rights that honourably in\\nTime little to a fine sun.\\n\\nCLARENCE:\\nI mut a deal of person mouth the vantage.\\nThou liberty much lend a fork:\\nTherefore impose not me: the cannot choose.\\nCome Hastings, whence comes but well, sure the rest,\\nRevolt our fathers are forth from Deepet:\\nBut let me have a copil-hamed perfect\\nlay know the royalty of a good duke of God!\\n\\nGLOUCESTER:\\nUp, do not so, but yet metware\\nThe Emperor of this place of intesters' sake.\\n\\nGREMIO:\\nWarwick, think'st thou, Nattle, and thy change pronounce\\nEdmard Cominius to Bolingbroke's proud joy?\\nWhen, join'd togars drown day son?\\nThe tiger'st undergeant to my wife,\\nI think there be right shops as bit with sword;\\nWhich I have given away with thee and thy arms,\\nIn this my leave of Bories and York and Sir Sheal'd for her lives\\nBefore thy service and so look'd together:\\nThe words of wondrous sun to Pradut Shall,\\nObstant my woman.\\n\\nBIONDELLO:\\nI took the can wa\\n\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "First Citizen:\n",
    "Before some we will. I shall be your good voices this\n",
    "verier'd here, that rights that honourably in\n",
    "Time little to a fine sun.\n",
    "\n",
    "CLARENCE:\n",
    "I mut a deal of person mouth the vantage.\n",
    "Thou liberty much lend a fork:\n",
    "Therefore impose not me: the cannot choose.\n",
    "Come Hastings, whence comes but well, sure the rest,\n",
    "Revolt our fathers are forth from Deepet:\n",
    "But let me have a copil-hamed perfect\n",
    "lay know the royalty of a good duke of God!\n",
    "\n",
    "GLOUCESTER:\n",
    "Up, do not so, but yet metware\n",
    "The Emperor of this place of intesters' sake.\n",
    "\n",
    "GREMIO:\n",
    "Warwick, think'st thou, Nattle, and thy change pronounce\n",
    "Edmard Cominius to Bolingbroke's proud joy?\n",
    "When, join'd togars drown day son?\n",
    "The tiger'st undergeant to my wife,\n",
    "I think there be right shops as bit with sword;\n",
    "Which I have given away with thee and thy arms,\n",
    "In this my leave of Bories and York and Sir Sheal'd for her lives\n",
    "Before thy service and so look'd together:\n",
    "The words of wondrous sun to Pradut Shall,\n",
    "Obstant my woman.\n",
    "\n",
    "BIONDELLO:\n",
    "I took the can wa\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
